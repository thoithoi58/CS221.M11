{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:46.482509Z",
     "start_time": "2022-01-11T03:17:44.480695Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:46.498304Z",
     "start_time": "2022-01-11T03:17:46.486509Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read each line in train/test file and split word and its part-of-speech by '/' seperator\n",
    "def read(file):\n",
    "    with open(file, 'r', encoding='utf8') as f:\n",
    "        file = f.read().splitlines()\n",
    "    data = [[] for _ in range(len(file))]\n",
    "    for idx, i in enumerate(file):\n",
    "        a = i.split()\n",
    "        for j in a:\n",
    "            tmp = (j.rsplit('/',1))\n",
    "            data[idx].append((tmp[0], tmp[1]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:46.528840Z",
     "start_time": "2022-01-11T03:17:46.501611Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load training corpus and testing file to memory\n",
    "train_set = read('C:/Users/owcap/Documents/Learning/CS221/Project/corpus/train.txt')\n",
    "test_set = read('C:/Users/owcap/Documents/Learning/CS221/Project/corpus/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:46.592837Z",
     "start_time": "2022-01-11T03:17:46.580839Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create list of train and test tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "test_tagged_words = [tup[0] for sent in test_set for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:47.113342Z",
     "start_time": "2022-01-11T03:17:47.097338Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "#Let's check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:47.534670Z",
     "start_time": "2022-01-11T03:17:47.522662Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:48.110290Z",
     "start_time": "2022-01-11T03:17:48.094392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028\n"
     ]
    }
   ],
   "source": [
    "#Let's check how many words are present in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging using Hidden Markov Model (HMM)\n",
    "\n",
    "We'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n",
    "In other words, to every **word w**, assign **the tag t** that maximises the likelihood **$P(t|w)$**. \n",
    "\n",
    "Since $$P(t|w) = {{P(w|t)*P(t)} \\over {P(w)}}$$ \n",
    "After ignoring $P(w)$, we have to compute $P(w|t)$ and $P(t)$.\n",
    "\n",
    "Now:\n",
    "* **$P(w|t)$: is the emission probability** of a given word for a given tag. This can be computed based on the fraction of given word for given tag to the total count of that tag, ie: $$P(w|t) = count(w, t) \\over count(t)$$. However, if the training data is small and not cover enough cases, the emission probability can lead to zero. Therefore we need to use a method called **Laplace smoothing** which adds a value to the numerator to prevent zero probability. The equation above can be written:\n",
    "$$P(w|t) = count(w, t) + 1 \\over count(t)+ N$$  which N is the number of unique tags in training data\n",
    "\n",
    "\n",
    "\n",
    "* **$P(t)$: is the probability of tag (also transition probability)**, and in a tagging task, we assume that a tag will depend only on the previous tag (Markov order 1 assumption). In other words, the probability of say a tag being NOUN will depend only on the previous tag $t(n-1)$. So for e.g. if $t(n-1)$ is a ADJ, then $t(n)$ is likely to be an NOUN since adjectives often precede a noun (blue coat, tall building etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Viterbi POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute emission probabilties for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:49.394300Z",
     "start_time": "2022-01-11T03:17:49.374509Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compute emission probability for a given word for a given tag\n",
    "def word_given_tag(word, tag, train_bag=train_tagged_words):\n",
    "    taglist = [pair for pair in train_bag if pair[1] == tag]\n",
    "    #Use laplace smoothing to prevent zero probability\n",
    "    tag_count = len(taglist) + unique_tags\n",
    "    w_in_tag = [pair[0] for pair in taglist if pair[0] == word]\n",
    "    word_count_given_tag = len(w_in_tag) + 1\n",
    "    return (word_count_given_tag, tag_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute transition probabilties for a given tag and previous tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:50.293953Z",
     "start_time": "2022-01-11T03:17:50.278125Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compute transition probabilities of a previous and next tag\n",
    "def t2_given_t1(t2,t1,train_bag=train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    t1_tags = [tag for tag in tags if tag==t1]  \n",
    "    count_of_t1 = len(t1_tags) + unique_tags \n",
    "    t2_given_t1 = [tags[index+1] for index in range(len(tags)-1) if tags[index] == t1 and tags[index+1] == t2] \n",
    "    count_t2_given_t1 = len(t2_given_t1) + 1  \n",
    "    return(count_t2_given_t1,count_of_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:51.007123Z",
     "start_time": "2022-01-11T03:17:50.974639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 201)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_given_t1('NOUN','ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:51.658464Z",
     "start_time": "2022-01-11T03:17:51.284533Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:52.171240Z",
     "start_time": "2022-01-11T03:17:52.151248Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:54.322963Z",
     "start_time": "2022-01-11T03:17:54.269412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>DET</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADP</th>\n",
       "      <th>AUX</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>PRON</th>\n",
       "      <th>SYM</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>PART</th>\n",
       "      <th>ADV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>0.067568</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.067568</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.016779</td>\n",
       "      <td>0.077181</td>\n",
       "      <td>0.218121</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.010067</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.489933</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.067114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.100592</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.183432</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.065089</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>0.124260</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.260355</td>\n",
       "      <td>0.053254</td>\n",
       "      <td>0.005917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.039801</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.606965</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.099502</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.009950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.047761</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.170149</td>\n",
       "      <td>0.011940</td>\n",
       "      <td>0.062687</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.241791</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.140299</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.071642</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.092537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.029605</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.088816</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.121711</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.049342</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.032895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.310078</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.100775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.017762</td>\n",
       "      <td>0.017762</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.097691</td>\n",
       "      <td>0.202487</td>\n",
       "      <td>0.063943</td>\n",
       "      <td>0.166963</td>\n",
       "      <td>0.046181</td>\n",
       "      <td>0.030195</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.277087</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.031972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.078652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.032086</td>\n",
       "      <td>0.347594</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>0.208556</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.042781</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.074866</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>0.049738</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.115183</td>\n",
       "      <td>0.107330</td>\n",
       "      <td>0.034031</td>\n",
       "      <td>0.094241</td>\n",
       "      <td>0.091623</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.049738</td>\n",
       "      <td>0.073298</td>\n",
       "      <td>0.151832</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.091623</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.081152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.111765</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.041176</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.070588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SCONJ       NUM         X       DET     PROPN       ADJ      VERB  \\\n",
       "SCONJ  0.067568  0.013514  0.013514  0.202703  0.027027  0.067568  0.054054   \n",
       "NUM    0.017857  0.017857  0.017857  0.017857  0.035714  0.071429  0.017857   \n",
       "X      0.050000  0.050000  0.050000  0.050000  0.050000  0.050000  0.050000   \n",
       "DET    0.003356  0.006711  0.003356  0.016779  0.077181  0.218121  0.046980   \n",
       "PROPN  0.005917  0.100592  0.005917  0.023669  0.183432  0.023669  0.023669   \n",
       "ADJ    0.024876  0.024876  0.004975  0.009950  0.019900  0.039801  0.014925   \n",
       "VERB   0.047761  0.005970  0.002985  0.170149  0.011940  0.062687  0.029851   \n",
       "ADP    0.006579  0.029605  0.006579  0.407895  0.088816  0.078947  0.121711   \n",
       "AUX    0.007752  0.023256  0.007752  0.100775  0.015504  0.139535  0.310078   \n",
       "NOUN   0.014210  0.005329  0.001776  0.017762  0.017762  0.003552  0.097691   \n",
       "CCONJ  0.022472  0.022472  0.011236  0.089888  0.146067  0.089888  0.101124   \n",
       "PRON   0.010695  0.005348  0.005348  0.021390  0.005348  0.032086  0.347594   \n",
       "SYM    0.052632  0.052632  0.052632  0.052632  0.157895  0.052632  0.052632   \n",
       "PUNCT  0.049738  0.013089  0.010471  0.115183  0.107330  0.034031  0.094241   \n",
       "PART   0.024390  0.048780  0.024390  0.024390  0.048780  0.048780  0.097561   \n",
       "ADV    0.047059  0.005882  0.005882  0.047059  0.011765  0.111765  0.300000   \n",
       "\n",
       "            ADP       AUX      NOUN     CCONJ      PRON       SYM     PUNCT  \\\n",
       "SCONJ  0.040541  0.040541  0.121622  0.013514  0.243243  0.013514  0.054054   \n",
       "NUM    0.053571  0.017857  0.142857  0.035714  0.017857  0.017857  0.482143   \n",
       "X      0.050000  0.050000  0.100000  0.050000  0.050000  0.050000  0.200000   \n",
       "DET    0.010067  0.020134  0.489933  0.003356  0.026846  0.003356  0.003356   \n",
       "PROPN  0.065089  0.017751  0.124260  0.076923  0.005917  0.023669  0.260355   \n",
       "ADJ    0.089552  0.014925  0.606965  0.024876  0.004975  0.004975  0.099502   \n",
       "VERB   0.241791  0.005970  0.140299  0.005970  0.104478  0.002985  0.071642   \n",
       "ADP    0.016447  0.016447  0.118421  0.003289  0.049342  0.003289  0.016447   \n",
       "AUX    0.031008  0.046512  0.031008  0.015504  0.069767  0.007752  0.038760   \n",
       "NOUN   0.202487  0.063943  0.166963  0.046181  0.030195  0.001776  0.277087   \n",
       "CCONJ  0.044944  0.056180  0.179775  0.011236  0.067416  0.011236  0.056180   \n",
       "PRON   0.053476  0.208556  0.085561  0.005348  0.042781  0.005348  0.074866   \n",
       "SYM    0.052632  0.052632  0.105263  0.052632  0.052632  0.052632  0.052632   \n",
       "PUNCT  0.091623  0.028796  0.049738  0.073298  0.151832  0.002618  0.091623   \n",
       "PART   0.024390  0.048780  0.390244  0.024390  0.024390  0.024390  0.024390   \n",
       "ADV    0.058824  0.029412  0.029412  0.017647  0.041176  0.005882  0.211765   \n",
       "\n",
       "           PART       ADV  \n",
       "SCONJ  0.013514  0.013514  \n",
       "NUM    0.017857  0.017857  \n",
       "X      0.050000  0.050000  \n",
       "DET    0.003356  0.067114  \n",
       "PROPN  0.053254  0.005917  \n",
       "ADJ    0.004975  0.009950  \n",
       "VERB   0.002985  0.092537  \n",
       "ADP    0.003289  0.032895  \n",
       "AUX    0.054264  0.100775  \n",
       "NOUN   0.021314  0.031972  \n",
       "CCONJ  0.011236  0.078652  \n",
       "PRON   0.005348  0.090909  \n",
       "SYM    0.052632  0.052632  \n",
       "PUNCT  0.002618  0.081152  \n",
       "PART   0.024390  0.097561  \n",
       "ADV    0.005882  0.070588  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viterbi Algorithm\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Given a sequence of words\n",
    "2. iterate through the sequence\n",
    "3. for each word (starting from first word in sequence) calculate the product of emission probabilties and transition probabilties for all possible tags.\n",
    "4. assign the tag which has maximum probability obtained in step 3 above.\n",
    "5. move to the next word in sequence to repeat steps 3 and 4 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:17:56.760387Z",
     "start_time": "2022-01-11T03:17:56.744900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vanilla Viterbi Algorithm\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    \n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['PUNCT', tag]\n",
    "#                 continue\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Viterbi Algorithm on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:24:52.154133Z",
     "start_time": "2022-01-11T03:24:42.500100Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(19522298)\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_set for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "tagged_seq = Viterbi(test_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:24:52.169861Z",
     "start_time": "2022-01-11T03:24:52.154133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Vanilla Viterbi Algorithm is - 61.81980374665478%\n"
     ]
    }
   ],
   "source": [
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]\n",
    "vanilla_viterbi_accuracy = len(check)/len(tagged_seq)\n",
    "print(\"The accuracy of the Vanilla Viterbi Algorithm is -\", str(vanilla_viterbi_accuracy*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
